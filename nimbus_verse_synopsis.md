# ğŸ“¦ NimbusVerse: End-to-End Data Pipeline in Docker

NimbusVerse is a production-grade, Dockerized data pipeline that integrates traditional data sources, cloud services, monitoring tools, and event-driven orchestration. This project transforms raw structured, semi-structured, and unstructured data into a well-monitored and automated ETL flow using Apache Airflow, Kafka, AWS, Prometheus, Grafana, and more.

---

## ğŸ”§ Features

- ğŸ›¢ï¸ Extracts data from MySQL and local directories
- ğŸ“Š Sends real-time metrics to Prometheus, visualized in Grafana
- ğŸ”„ Converts CSV to Parquet for scalability and optimized storage
- ğŸ§¾ Processes structured (CSV, SQL), semi-structured (JSON, XML), and unstructured (PDF, image, audio, video) files
- ğŸ“¡ Uses Kafka for triggering downstream jobs and sending process signals
- ğŸ“¬ Sends email notifications at defined checkpoints
- â˜ï¸ Uploads files to AWS S3, executes Glue jobs via Lambda
- ğŸ§  Maintains deduplication using a persistent CSV report
- ğŸ”„ Fully containerized using Docker with support for recovery wrappers

---

## ğŸ—‚ï¸ Folder Structure

```bash
NimbusVerse/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ airflow/        # Main DAGs and orchestration scripts
â”‚   â”œâ”€â”€ kafka/          # Kafka and Zookeeper Docker setup
â”‚   â”œâ”€â”€ scripts/        # Prometheus metric emission scripts
â”‚
â”œâ”€â”€ grafana/            # Independent Grafana dashboards
â”œâ”€â”€ mysql/              # Standalone MySQL database
â”œâ”€â”€ aws/                # AWS Lambda, Glue job code, configs
â””â”€â”€ README.md           # You're here
```

---

## ğŸ“ˆ Monitoring Dashboard

- Prometheus scrapes metrics from the `scripts/` module.
- Grafana visualizes metrics in real-time.
- Alerts can be extended via email, Slack, or Alertmanager.

---

## ğŸš€ Pipeline Flow

```mermaid
flowchart TD
    A[MySQL + Directory] --> B[Airflow DAG Trigger]
    B --> C[Kafka Signal: Start Upload]
    C --> D[Conversion to Parquet + Deduplication Check]
    D --> E[Prometheus: Metric Push]
    E --> F[Send Email Notification]
    F --> G[Kafka Signal: Cloud Upload]
    G --> H[Push to S3 + Write SIGNAL file]
    H --> I[AWS Lambda Triggered]
    I --> J[Glue Job Writes to RDS]
    J --> K[Write COMPLETION SIGNAL to S3]
    K --> L[Airflow DAG Waits for Final Signal]
    L --> M[Final Email + Cleanup]
```

---

## ğŸ”„ Deduplication Mechanism

A central `report.csv` tracks processed files with their timestamps. Every task checks this CSV to:

- Avoid duplicate processing
- Provide auditability
- Maintain efficiency

---

## ğŸ§ª Recovery Mechanism

- Each major script is wrapped with error recovery blocks
- Prevents task abortion from false alarms or retryable exceptions

---

## ğŸ§± Built With

- **Apache Airflow** â€“ Orchestration
- **Apache Kafka** â€“ Messaging & signaling
- **AWS (Lambda, Glue, S3, RDS)** â€“ Cloud processing
- **PySpark & Pandas** â€“ Data transformations
- **Prometheus & Grafana** â€“ Monitoring & dashboards
- **Docker** â€“ Containerized deployment

---

## ğŸŒ± Future Enhancements

- Push JSON/XML/PDF metadata to **DynamoDB**
- Extract and load unstructured data metadata into **Amazon Redshift**
- Add **CI/CD pipelines** using GitHub Actions
- Integrate **Alertmanager** for critical failures

---

## ğŸ“¬ Author

**Arin Dubey**

> Passionate about data engineering, automation, and cloud-native solutions.

---

## ğŸ“ License

MIT License â€” feel free to use and adapt this project with attribution.

---

## ğŸ“Œ Note

This project was built entirely within Docker containers, reflecting real-world deployment standards and enterprise-grade pipeline design.

---

For diagrams and a full project synopsis, refer to the `/docs/` folder or request a PDF version.

